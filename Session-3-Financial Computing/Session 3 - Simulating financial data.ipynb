{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **SIMULATING (FINANCIAL) DATA**\n",
    "---\n",
    "<img src=\"http://www.doc.ic.ac.uk/~afd/images/logo_imperial_college_london.png\" align = \"left\" width=200>\n",
    " <br><br><br><br>\n",
    " \n",
    "- Copyright (c) Antoine Jacquier, 2022. All rights reserved\n",
    "\n",
    "- Author: Jack Jacquier <a.jacquier@imperial.ac.uk>\n",
    "\n",
    "- Platform: Tested on Windows 10 with Python 3.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A **random number generator** is a device that generates numbers or symbols that cannot be reasonably predicted better than by a random chance.\n",
    "\n",
    "They can be true **hardware random number generators**, generating genuinely random numbers, or pseudorandom number generators. \n",
    "The former are fundamentally more accurate, but are harder to build; examples thereof are based on microscopic phenomena such as thermal noise, or quantum physics.\n",
    "The latter are algorithms generating numbers whose properties approximate those of truly random numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coin tosses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "random.randint(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[random.randint(0, 1) for _ in range(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 1000\n",
    "random_data = np.array([random.randint(0, 1) for _ in range(n)])\n",
    "plt.plot(range(n), random_data, '.')\n",
    "plt.show()\n",
    "print(\"Number of zeros:\", n-np.sum(random_data))\n",
    "print(\"Number of ones:\", np.sum(random_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The uniform random variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import scipy.stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "xx = np.linspace(scipy.stats.uniform.ppf(0.01),scipy.stats.uniform.ppf(0.99), 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(xx, scipy.stats.uniform.pdf(xx), 'k-', lw=2, label='pdf')\n",
    "plt.title(\"Probability density function of $\\mathcal{U}_{[0,1]}$\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If $X \\sim \\mathcal{U}_{[a, b]}$, then\n",
    "$$\n",
    "\\mathbb{E}[X] = \\frac{a+b}{2}\n",
    "\\qquad\\text{and}\\qquad\n",
    "\\mathbb{V}[X] = \\frac{(b-a)^2}{12}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "np.random.uniform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "np.random.uniform(size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us generate some more of these..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "variates = np.random.uniform(size=10000)\n",
    "plt.hist(variates, density=True, histtype='stepfilled', alpha=0.2, label='Empirical histogram')\n",
    "plt.plot(xs, scipy.stats.uniform.pdf(xs), 'k-', lw=2, label='pdf')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate 100,000 $\\text{Uniform}[a, b]$ random variates for $a = -1$, $b = 3$. Confirm that their mean and variance are as expected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random seed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One issue when working with pseudorandom data is the lack of **reproducibility**:\n",
    "When computing the random variates, we obtain different sequences. \n",
    "\n",
    "Even though they should have similar statistical properties, the difference may be enough to 'break' the results.\n",
    "\n",
    "*Example:* a neural network, with weights and biases initialised to pseudorandom numbers, may calibrate well for some pseudorandom sequences and fail to calibrate for others.\n",
    "\n",
    "In fact, pseudorandom sequences are (complex but deterministic) functions of the **random seed**, usually initialised using the system's clock.\n",
    "\n",
    "*Popular choice: 42 (The Hitchhiker's Guide to the Galaxy, by Douglas Adams)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "np.random.uniform(size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "np.random.uniform(size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normal (Gaussian) random variates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Gaussian distribution, because of the **Central Limit Theorem (CLT)**, is ubiquitous, especially in finance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If $X\\sim\\mathcal{N}((\\mu, \\sigma^2)$, then\n",
    "$$\n",
    "\\mathbb{E}[X] = \\mu\n",
    "\\qquad\\text{and}\\qquad\n",
    "\\mathbb{V}[X] = \\sigma^2.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the same set of axes, plot the PDFs of\n",
    "* $\\text{Normal}(\\mu = 0, \\sigma^2=0.2)$,\n",
    "* $\\text{Normal}(\\mu = 0, \\sigma^2=1)$,\n",
    "* $\\text{Normal}(\\mu = 0, \\sigma^2=5)$,\n",
    "* $\\text{Normal}(\\mu = -2, \\sigma^2=0.5)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "xx = np.linspace(-10., 10., 1000)\n",
    "X = scipy.stats.norm(loc=0., scale=np.sqrt(.2))\n",
    "plt.plot(xx, X.pdf(xx), '-', lw=2, label='$\\mu=0, \\sigma^2=0.2$')\n",
    "X = scipy.stats.norm(loc=0., scale=np.sqrt(1.))\n",
    "plt.plot(xx, X.pdf(xx), '-', lw=2, label='$\\mu=0, \\sigma^2=1$')\n",
    "X = scipy.stats.norm(loc=0., scale=np.sqrt(5.))\n",
    "plt.plot(xx, X.pdf(xx), '-', lw=2, label='$\\mu=0, \\sigma^2=5$')\n",
    "X = scipy.stats.norm(loc=-2., scale=np.sqrt(.5))\n",
    "plt.plot(xx, X.pdf(xx), '-', lw=2, label='$\\mu=-2, \\sigma^2=0.5$')\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lindeberg&ndash;L&eacute;vy Central Limit Theorem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $\\{X_1, \\ldots, X_n\\}$ be a sequence of iid. random variables with $\\mathbb{E}[X_i] = \\mu$ and $\\mathbb{V}[X_i] = \\sigma^2 < \\infty$ and\n",
    "$$\n",
    "S_n := \\frac{X_1 + \\ldots + X_n}{n}.\n",
    "$$\n",
    "\n",
    "**Law of large numbers:** $S_n)_{n\\geq 1}$ converges in probability and almost surely to $\\mu$ as $n \\uparrow \\infty$, and \n",
    "$$\n",
    "\\frac{\\sqrt{n}}{\\sigma}(S_n - \\mu) \\overset{distribution}{\\longrightarrow} \\mathcal{N}(0, 1).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*How can we, using the CLT, produce an approximation of a standard Normal random variable out of twelve i.i.d. $\\text{Uniform}[0, 1]$ random variables?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Implement a function `normal_twelve_uniforms` that generates `N` standard normal random variates using the 12-Uniform method, and generate 10,000 such variates. \n",
    "Confirm that their mean and variance are as expected. \n",
    "Plot a histogram and confirm that it looks roughly normal.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Name one disadvantage of using the twelve-Uniform method to approximate a standard Gaussian.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Box&ndash;Muller transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose $U_1$ and $U_2$ are independent samples chosen from the uniform distribution on the unit interval $[0, 1]$. Let\n",
    "$$Z_0 = R \\cos(\\Theta) = \\sqrt{-2\\ln U_1}\\cos(2\\pi U_2),$$\n",
    "$$Z_1 = R \\sin(\\Theta) = \\sqrt{-2\\ln U_1}\\sin(2\\pi U_2).$$\n",
    "Then $Z_0$ and $Z_1$ are independent random variables with a standard normal distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Write a function `box_muller` that, given an even number of standard uniform random variates, transforms them to standard normal variates.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Write a function `normal_box_muller` that uses `box_muller` to generate a given number of standard normal variates.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Cholesky decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Cholesky decomposition of a symmetric positive-definite matrix $\\mathbf{A}$ is a decomposition of the form\n",
    "$$\\mathbf{A} = \\mathbf{L}\\mathbf{L}^\\intercal,$$\n",
    "where $\\mathbf{L}$ is a lower triangular matrix.\n",
    "\n",
    "This can be implemented with `numpy.linalg.cholesky`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Example:* \n",
    "Let $\\mathbf{\\Sigma} = \\mathbf{L}\\mathbf{L}^\\intercal \\in \\mathbb{R}^{n \\times n}$ be a covariance matrix (i.e. a symmetric positive-definite matrix). Let $\\mathbf{X} \\in \\mathbb{R}^n$ be a random vector. Let $\\mathbf{Y} = \\mathbf{L}\\mathbf{X}$.\n",
    "\n",
    "Then\n",
    "$$\\mathbb{V}[\\mathbf{Y}] = \\mathbb{V}[\\mathbf{L}\\mathbf{X}] = \\mathbf{L}\\mathbb{V}[\\mathbf{X}]\\mathbf{L}^\\intercal.$$\n",
    "\n",
    "In particular, if $\\mathbf{X} \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I}_n)$, then $\\mathbf{Y} \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{\\Sigma})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*We can use the Cholesky decomposition to produce from uncorrelated standard normal variates normal variates with any given variance&ndash;covariance structure.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the covariance matrix is\n",
    "$$\\begin{pmatrix} 25 & -7.5 \\\\ -7.5 & 9 \\end{pmatrix}$$\n",
    "what are the variances and standard deviations of the two variables and what is the correlation between them?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Write a function `multivariate_normal` that generates $n$ normal random vectors with a given covariance matrix $\\mathbf{\\Sigma}$. The dimensionality of the normal random vectors is given by the size of $\\mathbf{\\Sigma}$.\n",
    "\n",
    "- Generate 10,000 two-dimensional random vectors with the covariance matrix\n",
    "$$\\begin{pmatrix} 25 & -7.5 \\\\ -7.5 & 9 \\end{pmatrix}.$$\n",
    "\n",
    "- Confirm that the generated data has the right covariance structure.\n",
    "\n",
    "- Plot the results.\n",
    "\n",
    "- Produce a histogram for each of the two dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Note*\n",
    "We have shown how Cholesky decomposition can be used to create correlated normal random variates from uncorrelated normal random variates. In Python we don't have to do this manually. We can use `numpy.random.multivariate_normal` instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "variates = np.random.multivariate_normal(\n",
    "    mean=[0, 0],\n",
    "    cov=[[25., -7.5], [-7.5, 9.]],\n",
    "    size=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "print(\"Shape: \", np.shape(variates))\n",
    "print(\"Covariance matrix: \", np.cov(variates.T))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random walk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For $n \\in \\mathbb{N}^*$, let\n",
    "\\begin{equation*}\n",
    "X_n = \\left\\{\n",
    "        \\begin{array}{ll}\n",
    "          +1, & \\hbox{with probability $\\frac{1}{2}$,} \\\\\n",
    "          -1, & \\hbox{with probability $\\frac{1}{2}$,}\n",
    "        \\end{array}\n",
    "      \\right.\n",
    "\\end{equation*}\n",
    "thus each $X_n$ is a Bernoulli random variable.\n",
    "\n",
    "Let $Y_0 := 0$ and, for $n \\in \\mathbb{N}^*$, let\n",
    "\\begin{equation*}\n",
    "Y_n := \\sum_{i=1}^n X_i.\n",
    "\\end{equation*}\n",
    "\n",
    "The process $(Y_n)_{n\\geq 0}$ is a **symmetric random walk**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each choice of nonnegative integers $0 = i_0 < i_1 < \\ldots < i_m$, $m \\in \\mathbb{N}^*$, \n",
    "the increments\n",
    "\\begin{equation*}\n",
    "Y_{i_{j+1}} - Y_{i_j} = \\sum_{i = i_j + 1}^{i_{j+1}} X_i,\n",
    "\\end{equation*}\n",
    "for $j \\in \\{0, 1, \\ldots, m - 1\\}$, are independent random variables, and note that\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbb{E}[Y_{i_{j+1}} - Y_{i_j}] & = \\mathbb{E}\\left[\\sum_{i = i_j + 1}^{i_{j+1}} X_i\\right] = \\sum_{i = i_j + 1}^{i_{j+1}} \\mathbb{E}[X_i] = \\sum_{i = i_j + 1}^{i_{j+1}} 0 = 0,\\\\\n",
    "\\mathbb{V}[Y_{i_{j+1}} - Y_{i_j}] & = \\mathbb{V}\\left[\\sum_{i = i_j + 1}^{i_{j+1}} X_i\\right] = \\sum_{i = i_j + 1}^{i_{j+1}} \\mathbb{V}[X_i] = \\sum_{i = i_j + 1}^{i_{j+1}} 1 = i_{j+1} - i_j.\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Furthermore, for each $n \\in \\mathbb{N}^0$,\n",
    "\\begin{equation*}\n",
    "\\mathbb{E}[Y_n] = 0, \\quad \\mathbb{V}[Y_n] = n.\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a given $N \\in \\mathbb{N}^*$, define the scaled symmetric random walk:\n",
    "\\begin{equation*}\n",
    "Z_t^{(N)} := \\frac{1}{\\sqrt{N}} Y_{Nt}\n",
    "\\end{equation*}\n",
    "for all $t \\in \\left\\{0, \\frac{1}{N}, \\frac{2}{N}, \\ldots, \\frac{N}{N}, \\frac{N+1}{N}, \\ldots\\right\\} =: T^{(N)}$, i.e. such $t$ that make $Nt$ a nonnegative integer, ensuring that $Y_{Nt}$ is well defined.\n",
    "\n",
    "For this process, for $s, t \\in T^{(N)}$, $s \\leq t$,\n",
    "\\begin{equation*}\n",
    "\\mathbb{E}[Z_t^{(N)} - Z_s^{(N)}] = 0, \\quad \\mathbb{V}\\left[Z_t^{(N)} - Z_s^{(N)}\\right] = t - s,\n",
    "\\end{equation*}\n",
    "and\n",
    "\\begin{equation*}\n",
    "\\mathbb{E}[Z_t^{(N)}] = 0, \\quad \\mathbb{V}\\left[Z_t^{(N)}\\right] = t.\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simulate $Z_t^{(N)}$ over $t \\in [0, 1]$ for $N \\in \\{5, 1000, 100000\\}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time interpolation\n",
    "\n",
    "We can turn the discrete-time process $Z^{(N)}$ into a continuous-time stochastic process by linear interpolation: for $t \\in [0, +\\infty)$, define\n",
    "$$\n",
    "\\hat{W}_t^{(N)} := Z_{\\frac{n}{N}}^{(N)} + \\left(t - \\frac{n}{N}\\right) \\left[Z_{\\frac{n+1}{N}}^{(N)} - Z_{\\frac{n}{N}}^{(N)}\\right],\n",
    "$$\n",
    "where $n \\in \\mathbb{N}^0$ is such that $\\frac{n}{N} \\leq t < \\frac{n+1}{N}$ (clearly it is unique, so $\\hat{W}_t^{(N)}$ is well defined)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can prove, using the central limit theorem that, for $s, t \\in [0, +\\infty)$, $s \\leq t$, the distribution of $\\hat{W}_t^{(N)} - \\hat{W}_s^{(N)}$ approaches normal with mean 0 and variance $t - s$ as $N \\uparrow +\\infty$.\n",
    "In this limit, the process $\\hat{W}^{(N)}$ becomes a **Brownian motion**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
