{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b1301dd-b6d1-4ea1-91c3-46a689369581",
   "metadata": {},
   "source": [
    "# **Intro to Financial Computing with Numpy**\n",
    "## **MSc in Mathematics and Finance 2025-2026**\n",
    "---\n",
    "<img src=\"Imperial_logo.png\" align = \"left\" width=250>\n",
    " <br><br><br> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c952019b-431e-4ac4-81fc-d944f2372950",
   "metadata": {},
   "source": [
    "# What Is Financial Computing?\n",
    "\n",
    "**Financial computing** is the intersection of **finance**, **mathematics**, and **computer science**.  \n",
    "It focuses on using computational methods, algorithms, and numerical techniques to **model, analyze, and optimize financial systems**.\n",
    "\n",
    "> In short:  \n",
    "> **Financial computing** is the use of programming, numerical analysis, and data-driven modeling to solve complex financial problems.\n",
    "\n",
    "---\n",
    "\n",
    "## Key Topics in Financial Computing\n",
    "\n",
    "| Area | Description | Common Methods / Tools |\n",
    "|------|--------------|------------------------|\n",
    "| **Numerical Methods for Finance** | Solving pricing and risk equations computationally | Monte Carlo simulation, finite differences |\n",
    "| **Portfolio Optimization** | Balancing risk and return | Linear algebra, quadratic programming |\n",
    "| **Time Series & Econometrics** | Modeling and forecasting market behavior | ARIMA, GARCH, Kalman filters |\n",
    "| **Machine Learning in Finance** | Using data-driven models for prediction | PCA, regression, neural networks |\n",
    "| **High-Performance Computing** | Accelerating large-scale simulations | Parallel computing, GPU acceleration |\n",
    "| **Financial Databases & Systems** | Managing and analyzing financial data | SQL, kdb+, time-series databases |\n",
    "| **Risk Management** | Quantifying and mitigating financial risk | Value-at-Risk (VaR), stress testing |\n",
    "| **Algorithmic & Quantitative Trading** | Automating trading decisions | Backtesting, optimization, execution systems |\n",
    "\n",
    "---\n",
    "\n",
    "## Example Problems Solved with Financial Computing\n",
    "\n",
    "1. **Option Pricing**  \n",
    "   Estimate the fair value of derivatives using Monte Carlo or finite difference methods.\n",
    "\n",
    "2. **Portfolio Optimization**  \n",
    "   Use linear algebra to find the optimal portfolio weights that minimize variance for a target return.\n",
    "\n",
    "3. **Risk Simulation**  \n",
    "   Model thousands of market scenarios to estimate Value at Risk (VaR) or Expected Shortfall.\n",
    "\n",
    "4. **Factor Modeling**  \n",
    "   Apply PCA to uncover common factors driving asset returns.\n",
    "\n",
    "5. **Algorithmic Trading**  \n",
    "   Implement automated trading strategies that react to real-time market data.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "## Career Paths\n",
    "\n",
    "| Role | Description |\n",
    "|------|--------------|\n",
    "| **Quantitative Analyst (\"Quant\")** | Develops pricing and risk models using mathematics and code |\n",
    "| **Algorithmic Trader / Quant Trader** | Designs and implements automated trading strategies |\n",
    "| **Quant Developer** | Implements and maintains quantitative libraries with strong focus in efficiency |\n",
    "| **Risk Modeler** | Simulates financial scenarios to estimate exposure |\n",
    "| **Financial Data Scientist** | Uses ML and data analysis for forecasting and insights |\n",
    "| **Software Engineer (Finance / FinTech)** | Builds large-scale trading, analytics, or risk systems |\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3f54ff-4618-4c9d-89c5-7e63c73762f3",
   "metadata": {},
   "source": [
    "### An example with Numpy: Principal Component Analysis (PCA)\n",
    "\n",
    "PCA is not only a practical tool for dimensionality reduction but also a mathematically elegant transformation with powerful properties.\n",
    "\n",
    "Let’s recall the core idea:\n",
    "\n",
    "Given a dataset $X=(X_1,...,X_N) \\in \\mathbb{R}^{T \\times N} $, where each column $X_i\\in\\mathbb{R}^T$ is a **standardized variable** (e.g. $\\mathbb{E}[X_i]=0$ and $\\mathbb{V}[X]=1$)\n",
    "\n",
    "1. Compute the sample covariance matrix:\n",
    "   $\n",
    "   \\Sigma = \\frac{1}{T-1} X^\\top X \n",
    "   $ **Note:** this holds due to $\\mathbb{E}[X_i]=0$\n",
    "2. Find its eigenvalues and eigenvectors:\n",
    "   $\n",
    "   \\Sigma v_i = \\lambda_i v_i\n",
    "   $\n",
    "3. The eigenvectors $ v_i $ form an **orthogonal basis**, and the eigenvalues $ \\lambda_i $ measure the variance explained by each component.\n",
    "\n",
    "The transformation into principal components is:\n",
    "\n",
    "$\n",
    "Z = X V\n",
    "$\n",
    "\n",
    "where $ V = [v_1, v_2, \\dots, v_N] $ is the eigenvector matrix.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ba1b6724-ed9a-4808-88fa-ab75646e05b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# define number of assets an number of observations to be used\n",
    "np.random.seed(0)\n",
    "n_assets = 6\n",
    "n_obs = 300\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9571656-b2fc-40a0-b78c-d2e4a316d6c0",
   "metadata": {},
   "source": [
    "#### Next we construct a simmetric unit covariance matrix we can sample from. To do so, we use tha fact that given any square matrix $A$, we have that $B=AA^T$ is simmetric and $\\Sigma=D^{-1}BD^{-1}$ is a valid correlation matrix i.e. represents a valid unit variance random vector. $D$ is a diagonal matrix where $Diag(D)=\\sqrt{Diag(\\Sigma)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c5f7586c-809b-4bd4-8010-47ed904c693a",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.random.randn(n_assets, n_assets)\n",
    "\n",
    "B = A @ A.T\n",
    "D_inverse=np.zeros((n_assets,n_assets))\n",
    "np.fill_diagonal(D_inverse,1.0/np.sqrt(np.diag(B)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5d7f7054-fad4-4929-a436-43fbecde2bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix=D_inverse@B@D_inverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9f10bf81-640b-4daf-a9eb-104f8a72c169",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.00000000e+00,  1.92492797e-01,  8.43602845e-01,\n",
       "         1.36037434e-01,  3.51757759e-01, -6.85435416e-01],\n",
       "       [ 1.92492797e-01,  1.00000000e+00,  2.22128486e-01,\n",
       "         7.03232326e-04,  7.52011378e-01, -1.12625668e-01],\n",
       "       [ 8.43602845e-01,  2.22128486e-01,  1.00000000e+00,\n",
       "         1.23521175e-01,  5.70852956e-01, -3.61183607e-01],\n",
       "       [ 1.36037434e-01,  7.03232326e-04,  1.23521175e-01,\n",
       "         1.00000000e+00,  1.88198837e-01,  4.15599665e-02],\n",
       "       [ 3.51757759e-01,  7.52011378e-01,  5.70852956e-01,\n",
       "         1.88198837e-01,  1.00000000e+00, -2.22856363e-02],\n",
       "       [-6.85435416e-01, -1.12625668e-01, -3.61183607e-01,\n",
       "         4.15599665e-02, -2.22856363e-02,  1.00000000e+00]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ddf7197-661b-4f30-88c4-78883a3c741f",
   "metadata": {},
   "source": [
    "#### Now we are able to sample data from a standardized covariance matrix and compute eigenvalues and eigenvectors using linear algebra in numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "179ac838-2942-44ec-8782-2b326c13e906",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.random.multivariate_normal(np.zeros(n_assets), corr_matrix, size=n_obs)\n",
    "\n",
    "# Sample Covariance and eigendecomposition. We don't center the sample since we know it has mean zero.\n",
    "Sigma = np.cov(X, rowvar=False)\n",
    "eigvals, eigvecs = np.linalg.eig(Sigma)\n",
    "idx = np.argsort(eigvals)[::-1] ## we sort eigenvalues\n",
    "eigvals, eigvecs = eigvals[idx], eigvecs[:, idx]## reorganize eigenvalues and vectors from largest to smallest\n",
    "\n",
    "# Project data\n",
    "Z = X @ eigvecs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e8c8f416-35d7-41ee-9c3d-430b0610203b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.54597624, 1.6226915 , 0.93010472, 0.57740585, 0.14059149,\n",
       "       0.03915131])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eigvals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "13e75546-e6dd-4deb-a1c2-5274af536244",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.53225614,  0.33277084,  0.08866611,  0.06705293,  0.41924322,\n",
       "        -0.64640878],\n",
       "       [ 0.31755765, -0.43771811, -0.41144659, -0.49261712,  0.48596914,\n",
       "         0.24379103],\n",
       "       [ 0.54375882,  0.10067697,  0.06259671,  0.5663007 ,  0.05886629,\n",
       "         0.6050706 ],\n",
       "       [ 0.15787338, -0.33348731,  0.88512773, -0.27188807,  0.03350204,\n",
       "         0.07325034],\n",
       "       [ 0.43398327, -0.51376525, -0.18608315,  0.11074976, -0.62712354,\n",
       "        -0.32791371],\n",
       "       [-0.32698839, -0.5588936 ,  0.02921729,  0.58817273,  0.43611644,\n",
       "        -0.20908892]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eigvecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2b49a875-cbe0-4574-8630-a0ffd2b10fa2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-2.04672864,  1.47990066,  0.06557973, -0.16054987, -0.41075018,\n",
       "        -0.30959489],\n",
       "       [ 2.80756332,  2.40149272, -0.05829031, -0.32386218, -0.52398012,\n",
       "         0.13610122],\n",
       "       [ 2.62166476, -0.0368768 , -1.09668498,  0.17547574, -0.19718017,\n",
       "        -0.25197741],\n",
       "       ...,\n",
       "       [ 2.09965931, -0.73675937, -1.0672616 , -0.36559628,  0.29107504,\n",
       "        -0.04524205],\n",
       "       [ 1.51409323, -1.68179175, -0.45673445, -0.08673606,  0.58090747,\n",
       "         0.03346729],\n",
       "       [ 0.12991791, -1.14627781, -0.576083  , -0.41559677, -0.14591403,\n",
       "        -0.17340011]], shape=(300, 6))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Z"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153b970b-099f-48c0-9f9b-501177f4846b",
   "metadata": {},
   "source": [
    "## Orthogonality of Principal Components\n",
    "\n",
    "One of the key properties of PCA is that the **principal components are orthogonal**.\n",
    "\n",
    "This means:\n",
    "$\n",
    "v_i^\\top v_j = 0 \\quad \\text{for } i \\neq j\n",
    "$\n",
    "and consequently, their scores (columns of $Z $) are **uncorrelated**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "db91d93a-9c59-44b8-abef-92d15536ec9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eigenvector dot products (should be close to identity):\n",
      " [[ 1.00000000e+00 -2.97951823e-16 -2.20679960e-16 -2.84413326e-16\n",
      "  -3.15186944e-16 -5.93091998e-16]\n",
      " [-2.97951823e-16  1.00000000e+00 -8.56681178e-17  1.19399128e-16\n",
      "   8.72997566e-17  5.07834081e-16]\n",
      " [-2.20679960e-16 -8.56681178e-17  1.00000000e+00  5.60504253e-16\n",
      "  -7.14812048e-16 -1.96099858e-17]\n",
      " [-2.84413326e-16  1.19399128e-16  5.60504253e-16  1.00000000e+00\n",
      "   6.04311151e-16 -4.59662361e-16]\n",
      " [-3.15186944e-16  8.72997566e-17 -7.14812048e-16  6.04311151e-16\n",
      "   1.00000000e+00 -4.35974571e-17]\n",
      " [-5.93091998e-16  5.07834081e-16 -1.96099858e-17 -4.59662361e-16\n",
      "  -4.35974571e-17  1.00000000e+00]]\n",
      "\n",
      "Correlation matrix of principal components (should be close to diagonal):\n",
      " [[ 1.00000000e+00 -8.03059904e-17  1.49635758e-17 -2.36965101e-17\n",
      "  -1.06710883e-15 -1.71948943e-15]\n",
      " [-8.03059904e-17  1.00000000e+00 -2.43157667e-16 -6.69881955e-17\n",
      "  -8.00936178e-16  2.18633138e-15]\n",
      " [ 1.49635758e-17 -2.43157667e-16  1.00000000e+00  9.47151692e-16\n",
      "   7.52479366e-16  1.03812303e-15]\n",
      " [-2.36965101e-17 -6.69881955e-17  9.47151692e-16  1.00000000e+00\n",
      "   1.74659497e-15 -2.26065500e-15]\n",
      " [-1.06710883e-15 -8.00936178e-16  7.52479366e-16  1.74659497e-15\n",
      "   1.00000000e+00 -5.36436673e-15]\n",
      " [-1.71948943e-15  2.18633138e-15  1.03812303e-15 -2.26065500e-15\n",
      "  -5.36436673e-15  1.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "# Check orthogonality of eigenvectors\n",
    "orthogonality_matrix = eigvecs.T @ eigvecs\n",
    "print(\"Eigenvector dot products (should be close to identity):\\n\", orthogonality_matrix)\n",
    "\n",
    "# Check correlation among principal component scores\n",
    "corr_Z = np.corrcoef(Z.T)\n",
    "print(\"\\nCorrelation matrix of principal components (should be close to diagonal):\\n\", corr_Z)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8be00e-1bca-4e94-8747-983edfff0415",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Observations\n",
    "\n",
    "- The eigenvectors are orthogonal → $ V^\\top V = I $.\n",
    "- The principal components $ Z = XV $ are *uncorrelated* random variables — this is evident from the nearly diagonal correlation matrix.\n",
    "\n",
    "This is what allows PCA to **decorrelate** correlated financial assets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "434c45ee-cc63-4640-afe8-97ca7c9d271f",
   "metadata": {},
   "source": [
    "##  Visualizing Variance Explained\n",
    "\n",
    "Let's visualize the **cumulative explained variance** to see how many components are needed to explain most of the total variability. To see this we just need to compare individual eigenvalues of components against the sum of all eigenvalues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e1cb6936-513a-45a8-bf0b-de963bf501a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "explained_var_ratio = eigvals / eigvals.sum()\n",
    "cum_var = np.cumsum(explained_var_ratio)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5056b4a9-a319-4ff9-9d09-d7f463737537",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.43476956, 0.27710269, 0.1588315 , 0.09860205, 0.02400843,\n",
       "       0.00668577])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "explained_var_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d063bcc8-89a2-49e0-be4e-5e8a2ba85e0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.43476956, 0.71187225, 0.87070375, 0.9693058 , 0.99331423,\n",
       "       1.        ])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cum_var"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c338fb25-864c-405d-94c5-3010225e4728",
   "metadata": {},
   "source": [
    "### Q? Can we use PCA if the observations are not Gaussian?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd1e822-3e88-4d4a-a24f-ac9fe0df9652",
   "metadata": {},
   "source": [
    "# When Is PCA Optimal, and How Does Non-Gaussianity Affect It?\n",
    "\n",
    "## 1. Optimality of PCA\n",
    "\n",
    "Principal Component Analysis (PCA) finds an **orthogonal linear transformation** that captures the maximum possible variance in the smallest number of components.\n",
    "\n",
    "Formally, given zero-mean data $ X \\in \\mathbb{R}^{T \\times N} $:\n",
    "\n",
    "$\n",
    "\\Sigma = \\frac{1}{T-1} X^\\top X\n",
    "$\n",
    "\n",
    "PCA solves recursively:\n",
    "$\\mathbf{w}_{(1)} = \\arg\\max \\left\\{ \\frac{\\mathbf{w}^\\mathsf{T} \\mathbf{X}^\\mathsf{T} \\mathbf{X w}}{\\mathbf{w}^\\mathsf{T} \\mathbf{w}} \\right\\}$\n",
    "\n",
    "\n",
    "The first principal component $w_1$ is the direction of **maximum variance**.  \n",
    "Each subsequent component is orthogonal to the previous ones and captures the next-largest share of variance.\n",
    "The k-th component can be found by subtracting the first k-1 principal components from $X$:\n",
    "\n",
    "$\\mathbf{\\hat{X}}_k = \\mathbf{X} - \\sum_{s = 1}^{k - 1} \\mathbf{X} \\mathbf{w}_{(s)} \\mathbf{w}_{(s)}^{\\mathsf{T}} $\n",
    "\n",
    "and then finding the weight vector which extracts the maximum variance from this new data matrix\n",
    "\n",
    "$\\mathbf{w}_{(k)}\n",
    "= \\mathop{\\operatorname{arg\\,max}}_{\\left\\| \\mathbf{w} \\right\\| = 1} \\left\\{ \\left\\| \\mathbf{\\hat{X}}_{k} \\mathbf{w} \\right\\|^2 \\right\\}\n",
    "= \\arg\\max \\left\\{ \\tfrac{\\mathbf{w}^\\mathsf{T} \\mathbf{\\hat{X}}_{k}^\\mathsf{T} \\mathbf{\\hat{X}}_{k} \\mathbf{w}}{\\mathbf{w}^T \\mathbf{w}} \\right\\}$\n",
    "\n",
    "\n",
    "#### Lucky for us, this boils down to finding eigenvalues and eigenvectors\n",
    "---\n",
    "\n",
    "\n",
    "## 2. When Data Are Non-Gaussian\n",
    "\n",
    "In practice, **financial data are rarely Gaussian**:\n",
    "- They often exhibit **heavy tails** (kurtosis > 3)\n",
    "- **Skewness**\n",
    "- **Volatility clustering**\n",
    "- **Nonlinear dependencies**\n",
    "\n",
    "In these cases:\n",
    "\n",
    "1. **Uncorrelated ≠ Independent**\n",
    "\n",
    "   PCA only ensures that the transformed variables are *uncorrelated*:\n",
    "   $\n",
    "   \\text{Cov}(Z_i, Z_j) = 0 \\quad (i \\ne j)\n",
    "   $\n",
    "   But for non-Gaussian data, they may still exhibit *higher-order* dependencies (nonlinear relationships).\n",
    "\n",
    "2. **PCA is not statistically optimal**\n",
    "\n",
    "   PCA minimizes *mean squared reconstruction error*, but not necessarily the true *information loss*.  \n",
    "   For non-Gaussian data, other directions may better separate structure or \"independent\" components.\n",
    "\n",
    "3. **Principal components may not correspond to true sources**\n",
    "\n",
    "   If your data were generated by mixing several latent independent signals (e.g., in financial factor models), PCA won’t necessarily recover those sources.  \n",
    "   Instead, methods like **Independent Component Analysis (ICA)** can separate them more effectively.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Financial Implications\n",
    "\n",
    "Financial returns are often **non-Gaussian**:\n",
    "- Fat-tailed (extreme events are more likely)\n",
    "- Asymmetric\n",
    "- Sometimes exhibit **nonlinear dependencies** across assets\n",
    "\n",
    "This means:\n",
    "- PCA can still be used as a **decorrelation** or **dimension-reduction** tool,  \n",
    "  but its components may not correspond to *true independent market factors*.\n",
    "- The first few components (especially the first one) still often capture the *market-wide risk* due to large common variance.\n",
    "- However, later components may mix nonlinear or tail behaviors not captured by simple covariance structure.\n",
    "\n",
    "---\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
